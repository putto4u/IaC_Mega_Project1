# 22장. 하이브리드 & 멀티 클라우드 DR 전략: AWS 장애 대응과 GCP 마이그레이션

## 1. 프로젝트 개요: 엔터프라이즈 인프라 생존 전략의 정점

본 프로젝트는 단일 클라우드에 의존하는 일반적인 아키텍처를 넘어, **물리 서버(On-Premise)**와 **이종 클라우드(Multi-Cloud)**를 유기적으로 결합한 엔터프라이즈급 **비즈니스 연속성 계획(BCP)**을 구현합니다.

이 프로젝트는 시스템 엔지니어링의 기술적 난이도와 실용성 측면에서 다음과 같은 독보적인 가치를 지닙니다.

1. **가시적인 DR(Disaster Recovery) 체험:** 추상적인 개념으로만 존재하던 '재해 복구'를 시각화합니다. 글로벌 클라우드(AWS)가 중단되는 즉시, 눈앞에 있는 **물리 서버(HQ)**가 트래픽을 받아내며 서비스가 살아나는 극적인 순간을 직접 구현합니다.
2. **멀티 클라우드 IaC(Infrastructure as Code)의 유연성:** AWS에서 시작된 서비스를 본사로 긴급 대피시킨 후, 다시 **GCP(Google Cloud Platform)**로 마이그레이션하는 전 과정을 **Terraform** 코드 하나로 통제합니다. 이는 특정 벤더에 종속되지 않는(Vendor Agnostic) 진정한 인프라 아키텍트의 역량을 증명합니다.
3. **최적화된 3단 방어 체계:** 비용 효율적인 '대기 모드'와 고가용성 '서비스 모드'를 배합하여, 비용과 안정성이라는 두 마리 토끼를 잡는 최적의 설계를 제시합니다.

---

## 2. 아키텍처 명세 및 시나리오 (3-Tier Operations)

### 2.1 운영 시나리오: 생존을 위한 3단계 전환

본 시스템은 위기 상황에 따라 다음과 같이 3단계로 변신하며 서비스를 지속합니다.

1. **Phase 1 (Normal - AWS):** 평시 운영 단계. 서울 리전의 **AWS Auto Scaling** 그룹이 전 세계 트래픽을 안정적으로 처리합니다. 본사는 최소한의 리소스로 대기(Standby)합니다.
2. **Phase 2 (Emergency - HQ):** AWS 리전 장애 발생 시, **DNS Failover**가 즉시 작동하여 **본사(HQ)의 물리 서버 40대**로 트래픽을 우회시킵니다. 물리 서버가 '최후의 보루'로서 서비스를 지탱합니다.
3. **Phase 3 (Migration - GCP):** 장기 장애에 대비하여 관리자의 명령(Command) 한 번으로 인프라를 **GCP**로 신속하게 배포하고 서비스를 이전합니다.

### 2.2 인프라 리소스 구성표

| 구분 | 위치 | 리소스 명칭 | 역할 및 상태 |
| --- | --- | --- | --- |
| **Control** | **본사** | `HQ-Control-Server` | **통합 지휘 서버** (Terraform, Ansible Master, VPN Gateway) |
| **Active** | **AWS** | `AWS-Web-ASG` | **[Phase 1]** 메인 서비스 (EC2 Auto Scaling + ALB) |
| **DR** | **본사** | `HQ-Physical-Nodes` | **[Phase 2]** AWS 중단 시 즉시 활성화되는 40대 물리 서버 |
| **Target** | **GCP** | `GCP-Web-MIG` | **[Phase 3]** 마이그레이션 명령 시 생성되는 구글 클라우드 자원 |
| **Network** | **Common** | `Route 53` | 헬스 체크 기반의 자동 Failover 및 수동 트래픽 전환 제어 |

---

## 3. 프로젝트 디렉토리 구조 설계

AWS, GCP, 그리고 본사(On-Premise)의 코드를 모듈화하여 관리 효율성을 극대화합니다.

**작업 위치:** `HQ-Control-Server` -> `/opt/enterprise-dr-project`

```text
/opt/enterprise-dr-project/
├── 00-inventory/           # 본사 물리 서버 자산 정보
│   └── hosts.ini
├── 01-hq-dr-ready/         # [Phase 2 준비] 본사 물리 서버 웹 서비스 대기 (Ansible)
│   ├── install_vbox.yml    # 가상화 엔진 프로비저닝
│   └── deploy_emergency.yml # 비상용 웹 페이지 배포
├── 02-aws-active/          # [Phase 1 메인] AWS 인프라 및 Failover 정책 (Terraform)
│   ├── main.tf
│   ├── route53.tf          # 핵심: Primary(AWS) -> Secondary(HQ) 자동 전환 설정
│   └── terraform.tfvars
├── 03-gcp-migration/       # [Phase 3 이전] GCP 마이그레이션 코드 (Terraform)
│   ├── main.tf             # GCP 인스턴스 및 로드밸런서 정의
│   └── terraform.tfvars
└── 04-control-scripts/     # 운영 자동화 스크립트
    └── switch_to_gcp.sh    # GCP 배포 및 DNS 전환 실행

```

---

## 4. [Phase 1] AWS 메인 구축 및 Failover 정책 수립

**목표:** 메인 서비스인 AWS 인프라를 구축하고, AWS 장애 감지 시 자동으로 본사로 트래픽을 넘기는 **Route 53 Failover Policy**를 구현합니다.

**작업 위치:** `/opt/enterprise-dr-project/02-aws-active`

**`route53.tf` (지능형 라우팅 정책)**

```hcl
# 1. Primary Record: AWS ALB (Active Mode)
resource "aws_route53_record" "service_endpoint" {
  zone_id = data.aws_route53_zone.main.zone_id
  name    = "www.enterprise-service.com"
  type    = "A"

  alias {
    name                   = aws_lb.main_alb.dns_name
    zone_id                = aws_lb.main_alb.zone_id
    evaluate_target_health = true
  }

  failover_routing_policy {
    type = "PRIMARY"
  }
  set_identifier  = "aws-main-region"
  health_check_id = aws_route53_health_check.aws_monitor.id
}

# 2. Secondary Record: HQ Public IP (DR Mode)
resource "aws_route53_record" "dr_endpoint" {
  zone_id = data.aws_route53_zone.main.zone_id
  name    = "www.enterprise-service.com"
  type    = "A"
  ttl     = "60"
  records = ["211.x.x.x"] # 본사 게이트웨이 공인 IP

  failover_routing_policy {
    type = "SECONDARY"
  }
  set_identifier = "hq-physical-center"
}

# 3. Health Check: AWS 생존 여부 감시
resource "aws_route53_health_check" "aws_monitor" {
  fqdn              = aws_lb.main_alb.dns_name
  port              = 80
  type              = "HTTP"
  failure_threshold = "3" # 3회 응답 실패 시 즉시 본사로 전환
  request_interval  = "10"
}

```

---

## 5. [Phase 2] 본사 DR 센터 가동 (Emergency Activation)

**시나리오:** AWS 리전 장애 발생 (Simulation: AWS 리소스 강제 중단).

이 단계에서 시스템은 관리자의 개입 없이 **자동으로 반응**합니다.

1. **장애 감지:** Route 53의 Health Check가 AWS ALB의 응답 없음을 감지합니다.
2. **DNS Failover:** DNS 응답이 AWS IP에서 **본사(HQ) 공인 IP**로 즉시 변경됩니다.
3. **물리 서버 활성화:**
* 트래픽이 본사 게이트웨이를 통과하여 내부 40대 물리 서버로 분산됩니다.
* 사용자의 브라우저에는 **"[EMERGENCY] HQ Physical DR Center Activated"**라는 메시지가 표시됩니다.
* **시각적 검증:** 실제 물리 서버(PC)의 터미널 로그에 외부 트래픽이 유입되는 것을 눈으로 직접 확인함으로써, 데이터센터 DR의 작동 원리를 체감합니다.



---

## 6. [Phase 3] 멀티 클라우드 전략: GCP로의 대이동 (Migration)

**시나리오:** 본사 물리 서버의 부하 분산 및 서비스 안정화를 위해 관리자가 **GCP로의 이전**을 결정합니다.

**작업 위치:** `/opt/enterprise-dr-project/03-gcp-migration`

이 단계는 **관리자의 의도(Command)**에 따라 수행되는 전략적 이동입니다.

**`main.tf` (GCP 인프라 프로비저닝)**

```hcl
provider "google" {
  project = "enterprise-dr-project"
  region  = "asia-northeast3" # GCP 서울 리전
}

# Google Cloud Managed Instance Group (AWS ASG 대응)
resource "google_compute_instance_template" "gcp_web_tpl" {
  name_prefix  = "gcp-web-"
  machine_type = "e2-micro"
  
  metadata_startup_script = "apt install -y nginx; echo 'Service Successfully Migrated to Google Cloud' > /var/www/html/index.html"
  
  # ... 네트워크 및 디스크 설정 ...
}

resource "google_compute_region_instance_group_manager" "gcp_mig" {
  name               = "gcp-web-mig"
  base_instance_name = "gcp-worker"
  target_size        = 2
  # ...
}

```

**마이그레이션 실행 절차:**

1. **GCP 인프라 배포:**
```bash
terraform init
terraform apply -auto-approve

```


* 수 분 내에 GCP 서울 리전에 웹 서버 클러스터와 로드밸런서가 생성됩니다.


2. **DNS 라우팅 변경 (Final Switch):**
* Route 53에서 도메인의 A 레코드를 **본사 IP**에서 생성된 **GCP Load Balancer IP**로 수정합니다.
* 트래픽 흐름이 `[사용자] -> [GCP Cloud]`로 변경되며 마이그레이션이 완료됩니다.



---

## 7. 프로젝트 결론 및 기술적 성취

이 프로젝트를 통해 우리는 단순한 서버 구축을 넘어 다음과 같은 고급 엔지니어링 역량을 확보했습니다.

1. **Infrastructure as Code (IaC) Mastery:** AWS와 GCP라는 서로 다른 클라우드 환경을 Terraform이라는 하나의 언어로 능수능란하게 다루는 **멀티 클라우드 운영 능력**을 입증했습니다.
2. **Resilient Architecture Design:** 클라우드 장애라는 극한 상황에서도 물리 서버를 활용해 서비스를 유지하는 **회복 탄력성(Resilience)** 높은 아키텍처를 설계했습니다.
3. **Hybrid Networking:** 공용 클라우드와 온프레미스 데이터센터를 DNS와 네트워크 레벨에서 유기적으로 연결하는 **하이브리드 네트워킹** 기술을 완성했습니다.

Next Level : Vagrant를 이용한 VirtualBox 내부 VM 프로비저닝 및 Docker Swarm/K8s 클러스터링
